{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f71bb0-36a4-401a-8e10-ce32cba3e46c",
   "metadata": {},
   "source": [
    "#Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e73cecf-066f-4cc4-9126-123c985b8510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.14 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-metal==1.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: opencv-python-headless in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: matplotlib in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-metal==1.1) (0.41.2)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-metal==1.1) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (68.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14) (2.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/gabrielschull/anaconda3/envs/facialrecognition/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.14 tensorflow-metal==1.1 opencv-python-headless matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33574ad1-70b4-4b2c-b38c-dbf522f957a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gabrielschull/anaconda3/envs/facialrecognition/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63584d24-aa83-4347-bfbe-dde9e01fb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149ee4c0-2323-43d4-9740-7170f0e15343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ac52a1-d8f1-463f-a068-5b150c18ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#Set GPU Memory Consumption Growth to avoid OOM errors -> wrong tensorflow version for this, not necessary though\n",
    "gpus = print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a27e082-476c-4f1d-934f-c7dde0b52e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd02a0ff-8dde-4e18-9e83-04e690c50687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dirs\n",
    "#os.makedirs(POS_PATH)\n",
    "#os.makedirs(NEG_PATH)\n",
    "#os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ef90d4-b11c-4d93-b8b6-13342abbd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting labelled faces in the wild dataset at http://vis-www.cs.umass.edu/lfw/ (as gzipped tar file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2831084c-cc2b-4174-b245-dcad866832ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncompress Tar Labelled Faces in the Wild dataser\n",
    "#!tar -xf lfw.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75d41a8-4947-4850-a50e-b8f292038e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move lfw imgs to the following repo data/negative\n",
    "#for directory in os.listdir('lfw'):\n",
    "   # for file in os.listdir(os.path.join('lfw', directory)):\n",
    "       # EX_PATH = os.path.join('lfw', directory, file)\n",
    "       # NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "       # os.replace(EX_PATH, NEW_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03008aa1-b77d-4914-bc4a-3abfcf794b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81cad31-de90-48e6-9f61-cafd78040c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect positive and anchor classes\n",
    "\n",
    "# cap = cv2.VideoCapture(0) #Establish a connection to the webcam\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read() \n",
    "    \n",
    "#     #cut down to 250x250px frame\n",
    "#     frame = frame[250:250+250,475:475+250,:]\n",
    "\n",
    "#     key = cv2.waitKey(1) & 0XFF\n",
    "\n",
    "#     #collect anchors\n",
    "#     if key == ord('a'):\n",
    "#         #create unique filepath\n",
    "#         imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "#         #write out anchor image\n",
    "#         cv2.imwrite(imgname, frame)\n",
    "#         break\n",
    "\n",
    "#     #collect positives\n",
    "#     if key == ord('p'):\n",
    "#         #create unique filepath\n",
    "#         imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "#         #write out positive image\n",
    "#         cv2.imwrite(imgname, frame)\n",
    "#         break\n",
    "        \n",
    "        \n",
    "#     #show image back to screen\n",
    "#     cv2.imshow('Image Collection', frame)\n",
    "\n",
    "#     #breaking if q hit on keyboard after 1 ms\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "# #release webcam & close the image show frame\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0f70a19-b7df-4a07-8fcd-06b0bdf93379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83c1983-f36e-4df8-82ff-453f240380e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 11:24:30.229030: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-02-28 11:24:30.229094: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-02-28 11:24:30.229105: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-02-28 11:24:30.229484: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-28 11:24:30.229539: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#Load and preprocess Images\n",
    "# get image directories\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(100)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(100)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d5cef7-4b64-453c-8da9-0b60e30fb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4958157c-0c42-4f76-a6dd-db2812f79edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data/anchor/cb0691d2-d2a2-11ee-944e-563466da705a.jpg'\n"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f3b69d-1e98-45e5-982f-f01fd14a9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing - scale and resize\n",
    "\n",
    "def preprocess(file_path):\n",
    "\n",
    "    # read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # load in the image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    # preprocessing steps - resize image to be 105x105x3\n",
    "    img = tf.image.resize(img, (105,105))\n",
    "    # scale image to be between 0 and 1\n",
    "    img = img / 255.0\n",
    "\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "953c918b-cfd0-473f-a6b4-b0da0eb99897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f433c730-5bba-4f5f-af92-1d9b01384056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labelled dataset\n",
    "# (anchor, positive) =>  1,1,1,1,1\n",
    "# (anchor, negative) => 0,0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b17a73-385a-4326-8d46-2070510e2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40a924fe-ba19-4545-846f-b04e3141982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "923e3db5-1f2f-46a4-85d8-e007dac9b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91196653-4e4a-442f-b61a-97d57ebd2bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'data/anchor/ccda33ec-d2a2-11ee-944e-563466da705a.jpg',\n",
       " b'data/positive/0bf032e8-d2a3-11ee-ab5a-563466da705a.jpg',\n",
       " 1.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3c78c86-4131-4e18-85df-bf21c3a41a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Train and Test Partition\n",
    "def preprocess_twin(input_img, validation_img, label):\n",
    "     return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a8174a3-82cd-4d01-9cd6-2b9dbbb0e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(105, 105, 3), dtype=float32, numpy=\n",
       " array([[[0.5155107 , 0.4645303 , 0.3900205 ],\n",
       "         [0.5423703 , 0.4908297 , 0.4163199 ],\n",
       "         [0.5439932 , 0.48516965, 0.40711176],\n",
       "         ...,\n",
       "         [0.2450937 , 0.17805351, 0.12922066],\n",
       "         [0.3038749 , 0.2332867 , 0.18622786],\n",
       "         [0.28197724, 0.211389  , 0.15648705]],\n",
       " \n",
       "        [[0.5273977 , 0.47641727, 0.40190747],\n",
       "         [0.5313926 , 0.48011208, 0.40426174],\n",
       "         [0.5510872 , 0.49226362, 0.40991068],\n",
       "         ...,\n",
       "         [0.18953587, 0.12651066, 0.0763706 ],\n",
       "         [0.20514213, 0.13561429, 0.08807529],\n",
       "         [0.282433  , 0.21240498, 0.16506605]],\n",
       " \n",
       "        [[0.5416767 , 0.49247035, 0.41263843],\n",
       "         [0.552381  , 0.5044818 , 0.41848743],\n",
       "         [0.5474345 , 0.488611  , 0.40625808],\n",
       "         ...,\n",
       "         [0.20026673, 0.13600767, 0.09598944],\n",
       "         [0.19299057, 0.12831803, 0.08311994],\n",
       "         [0.21570854, 0.15118939, 0.10553112]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.76807845, 0.72494113, 0.64650977],\n",
       "         [0.77447647, 0.7313392 , 0.65290785],\n",
       "         [0.7809902 , 0.73785293, 0.65942156],\n",
       "         ...,\n",
       "         [0.28819758, 0.2372172 , 0.30780542],\n",
       "         [0.32162198, 0.26700014, 0.3336668 ],\n",
       "         [0.33371785, 0.2740429 , 0.33595878]],\n",
       " \n",
       "        [[0.7788983 , 0.73576105, 0.6573296 ],\n",
       "         [0.7840137 , 0.74087644, 0.66244507],\n",
       "         [0.78580767, 0.7426704 , 0.66423905],\n",
       "         ...,\n",
       "         [0.290723  , 0.2397426 , 0.3138789 ],\n",
       "         [0.32020813, 0.26220492, 0.33591443],\n",
       "         [0.3181006 , 0.25955716, 0.33350676]],\n",
       " \n",
       "        [[0.77721316, 0.7340759 , 0.65564454],\n",
       "         [0.779005  , 0.73586774, 0.6574363 ],\n",
       "         [0.7941177 , 0.75098044, 0.67254907],\n",
       "         ...,\n",
       "         [0.29843506, 0.24745467, 0.3180429 ],\n",
       "         [0.32595044, 0.27497002, 0.3455583 ],\n",
       "         [0.3138211 , 0.26284072, 0.33342895]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(105, 105, 3), dtype=float32, numpy=\n",
       " array([[[0.55087817, 0.4842115 , 0.4057801 ],\n",
       "         [0.5575097 , 0.49084303, 0.41241163],\n",
       "         [0.5475257 , 0.48085904, 0.40242767],\n",
       "         ...,\n",
       "         [0.5572363 , 0.49056962, 0.41213825],\n",
       "         [0.5518141 , 0.48514742, 0.40671605],\n",
       "         [0.54957765, 0.48291096, 0.4044796 ]],\n",
       " \n",
       "        [[0.5520742 , 0.4854075 , 0.40697613],\n",
       "         [0.56528616, 0.49861947, 0.4201881 ],\n",
       "         [0.56252503, 0.49585837, 0.417427  ],\n",
       "         ...,\n",
       "         [0.5593237 , 0.4926571 , 0.4142257 ],\n",
       "         [0.57619053, 0.50952387, 0.4310925 ],\n",
       "         [0.5514473 , 0.4847806 , 0.40634924]],\n",
       " \n",
       "        [[0.56716686, 0.5005002 , 0.42206883],\n",
       "         [0.5637122 , 0.49704552, 0.41861412],\n",
       "         [0.56648   , 0.50061584, 0.41977683],\n",
       "         ...,\n",
       "         [0.5676204 , 0.50095373, 0.42252237],\n",
       "         [0.5719888 , 0.50532216, 0.4268908 ],\n",
       "         [0.56194925, 0.4952826 , 0.4168512 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.7706705 , 0.7432195 , 0.629494  ],\n",
       "         [0.77379626, 0.7455049 , 0.63233966],\n",
       "         [0.7919524 , 0.7527367 , 0.64685434],\n",
       "         ...,\n",
       "         [0.7914099 , 0.6937442 , 0.797479  ],\n",
       "         [0.78621453, 0.6920969 , 0.79405767],\n",
       "         [0.7803922 , 0.6862745 , 0.78823537]],\n",
       " \n",
       "        [[0.769948  , 0.742497  , 0.62877154],\n",
       "         [0.7811925 , 0.7529012 , 0.6397359 ],\n",
       "         [0.7895759 , 0.7503602 , 0.64447784],\n",
       "         ...,\n",
       "         [0.78656137, 0.6921903 , 0.7942778 ],\n",
       "         [0.7845739 , 0.6904562 , 0.79241705],\n",
       "         [0.7820795 , 0.6879619 , 0.78992265]],\n",
       " \n",
       "        [[0.77647066, 0.7490196 , 0.63529414],\n",
       "         [0.7809524 , 0.7526611 , 0.63949585],\n",
       "         [0.79393095, 0.75471526, 0.6488329 ],\n",
       "         ...,\n",
       "         [0.79038286, 0.6884221 , 0.80214757],\n",
       "         [0.78823537, 0.6862745 , 0.8000001 ],\n",
       "         [0.7960785 , 0.69411767, 0.8078432 ]]], dtype=float32)>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_twin(*example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46309a32-d3cf-4211-8b37-17cc4cc8100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataLoader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dfd36aa-5409-4d1c-b59f-43d5b7689ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad352880-afbd-42f7-8fbd-9543d81bc089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ShuffleDataset element_spec=(TensorSpec(shape=(105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e18f4d-c8c5-482b-819e-6398961c6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37428626-a2e5-45dd-9fa8-41286852e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b983bf94-4442-432e-a15e-093c68236f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e67e5993-0abb-4c72-8fbc-63c852894594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60f2b119-bd31-4ac2-b060-4046733956a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(105,105,3), name='input_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f132bf29-5746-4e8a-a9fb-1b685bdaf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Conv2D(64, (10,10), activation='relu')(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "981cfad9-26ea-4c73-aa3e-0b4f3f663173",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MaxPooling2D(64,(2,2), padding='same')(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c0edaf2-e0dc-4a5a-b22f-1a85a333dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "m2 = MaxPooling2D(64,(2,2), padding='same')(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7622534-ac8d-40e9-b938-4f4752db1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "m3 = MaxPooling2D(64,(2,2), padding='same')(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5314fa50-cc8b-4bad-b7e5-fbf0f0c1c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "f1 = Flatten()(c4)\n",
    "d1 = Dense(4096, activation='sigmoid')(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f81cc28-6d73-4198-9546-2fac7dece618",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dce62af7-2162-4f00-9a85-96f157ad9f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 105, 105, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 42, 42, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 21, 21, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 18, 18, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 9, 9, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38960448 (148.62 MB)\n",
      "Trainable params: 38960448 (148.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f3e2b4-e93f-4e3d-85d1-41849aa75773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding layer\n",
    "def make_embedding():\n",
    "    inp = Input(shape=(105,105,3), name='input_image')\n",
    "\n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64,(2,2), padding='same')(c1)\n",
    "\n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64,(2,2), padding='same')(c2)\n",
    "\n",
    "    # Third block\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64,(2,2), padding='same')(c3)\n",
    "\n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2aaebc10-1bbd-4bfc-9eb4-c2ecbb73ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3202097-0b10-49c8-9b1e-64a0b6fdd884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 105, 105, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 96, 96, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 48, 48, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 42, 42, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 21, 21, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 18, 18, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 9, 9, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38960448 (148.62 MB)\n",
      "Trainable params: 38960448 (148.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d68a7c31-567f-4935-bfba-4e5d963adb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build distance Layer\n",
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "\n",
    "    # Init inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    # Compare similarity\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "806888c8-297f-4f81-86db-4e3e5936943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "261981e4-6017-4a56-8447-e01f8f14afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1(anchor_embedding, validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70969111-5f08-420c-b249-b5ee3ad461a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(name='input_img', shape=(105,105,3))\n",
    "validation_image = Input(name='validation_img', shape=(105,105,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e3d0e51-a1b6-48dd-94b5-8611e90a6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_embedding = embedding(input_image)\n",
    "val_embedding = embedding(validation_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c7e40a5-1291-49ab-8550-0970e8551826",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_layer = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b81e0ca-c13c-4fd9-8e8a-067f00169028",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = siamese_layer(inp_embedding, val_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9800c2dc-32e8-4312-bff3-f1f4215093a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dense(1, activation='sigmoid')(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5144cb1-4a5e-4d0f-b6ea-f5847ec74c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_2')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5aa01874-561d-495c-9a77-1fe7ab9cdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2eefe3ec-7364-4801-8df8-dd5734af62a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 105, 105, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 105, 105, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " l1_dist_1 (L1Dist)          (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    4097      ['l1_dist_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79b2d901-b131-42e7-bc4f-456d4c093ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "\n",
    "    # Anchor image inputs in network\n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    # Validation image in network\n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Classificaiton layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c01afc15-f402-49be-baa9-7eb41cf026da",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87f30e60-ded4-4b9a-9748-a285993f9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 105, 105, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 105, 105, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " distance (L1Dist)           (None, 4096)                 0         ['embedding[2][0]',           \n",
      "                                                                     'embedding[3][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    4097      ['distance[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e9f91c2-20cb-4330-9b29-d186a2634016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "434c7f35-c7d7-4d37-b1c7-138324946232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Loss and Optimizer\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "903ba478-03b9-40b3-8171-32a9a87baeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = tf.keras.optimizers.legacy.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1cf7987-7c78-46a6-ae05-21fb155d7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish checkpoints\n",
    "checkpoint_dir = './training.checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0461fc3a-855e-46d7-86d8-aec36d60c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train step function\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get anchor and positive/negative image\n",
    "            X = batch[:2]\n",
    "            # Get label\n",
    "            y = batch[2]\n",
    "\n",
    "            # Forward pass\n",
    "            yhat = siamese_model(X, training=True)\n",
    "            #Calculate loss\n",
    "            loss = binary_cross_loss(y, yhat)\n",
    "        tf.print('loss:', loss)\n",
    "\n",
    "        # Calculate gradients\n",
    "        grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "        # Calculate updated weights and apply to siamese model\n",
    "        opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "        # Return loss\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b0881d8-0968-41ed-aa8c-a154a2fabad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training \n",
    "def train(data, EPOCHS):\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "            \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d26b457d-bcee-447a-a894-3e56a3a82da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f85695-1988-4c35-942c-bec115d98320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e93c33fd-8ddc-4c42-b58c-49c5e220f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "# Import metric calculations\n",
    "# Precision -> proportion of positive identifications were correct; Recall -> proportion of actual positives correctly identified\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bb2365b-a6f0-4989-b7e2-91ddd6b0836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_input_, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ff54517-20a4-454c-8d44-c011300a7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7983bf-2291-417b-9b3b-97de0abae064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87725f90-1400-44b2-b417-59103a241060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a46f6b-a78b-4886-a27e-d6a1cfbb7fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (facialrecognition)",
   "language": "python",
   "name": "facialrecognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
